{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk extension example using `Trainer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs requirements if you havenÂ´t already done it and sets up ipywidgets for datasets in sagemaker studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os \n",
    "import IPython\n",
    "if 'SAGEMAKER_TRAINING_MODULE' in os.environ:\n",
    "    !conda install -c conda-forge ipywidgets -y\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Sagemaker Session with local AWS Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From outside these notebooks, `get_execution_role()` will return an exception because it does not know what is the role name that SageMaker requires.\n",
    "\n",
    "To solve this issue, pass the IAM role name instead of using `get_execution_role()`.\n",
    "\n",
    "Therefore you have to create an IAM-Role with correct permission for sagemaker to start training jobs and download files from s3. Beware that you need s3 permission on bucket-level `\"arn:aws:s3:::sagemaker-*\"` and on object-level     `\"arn:aws:s3:::sagemaker-*/*\"`. \n",
    "\n",
    "You can read [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) how to create a role with right permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local aws profile configured in ~/.aws/credentials\n",
    "local_profile_name='hf-sm' # optional if you only have default configured\n",
    "\n",
    "# role name for sagemaker -> needs the described permissions from above\n",
    "role_name = \"SageMakerRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philipp to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::558105141721:role/SageMakerRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "try:\n",
    "    sess = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    import boto3\n",
    "    # creates a boto3 session using the local profile we defined\n",
    "    if local_profile_name:\n",
    "        os.environ['AWS_PROFILE'] = local_profile_name # setting env var bc local-mode cannot use boto3 session\n",
    "        #bt3 = boto3.session.Session(profile_name=local_profile_name)\n",
    "        #iam = bt3.client('iam')\n",
    "        # create sagemaker session with boto3 session\n",
    "        #sess = sagemaker.Session(boto_session=bt3)\n",
    "    iam = boto3.client('iam')\n",
    "    sess = sagemaker.Session()\n",
    "    # get role arn\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    \n",
    "\n",
    "\n",
    "print(role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Session prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/imdb/small/test/dataset.arrow', 'datasets/imdb/small/test/dataset_info.json', 'datasets/imdb/small/test/state.json', 'datasets/imdb/small/test/test_dataset.pt', 'datasets/imdb/small/train/dataset.arrow', 'datasets/imdb/small/train/dataset_info.json', 'datasets/imdb/small/train/state.json', 'datasets/imdb/small/training/train_dataset.pt', 'datasets/imdb/test/dataset.arrow', 'datasets/imdb/test/dataset_info.json', 'datasets/imdb/test/state.json', 'datasets/imdb/train/dataset.arrow', 'datasets/imdb/train/dataset_info.json', 'datasets/imdb/train/state.json']\n",
      "sagemaker-eu-central-1-558105141721\n",
      "eu-central-1\n"
     ]
    }
   ],
   "source": [
    "print(sess.list_s3_files(sess.default_bucket(),'datasets/')) # list objects in s3 under datsets/\n",
    "print(sess.default_bucket()) # s3 bucketname\n",
    "print(sess.boto_region_name) # aws region of sagemaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Since we are using the `.py` module directly from `huggingface/` we have to adjust our `sys.path` to be able to import our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface.estimator import HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Estimator with an Experiment\n",
    "\n",
    "[Metric Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)\n",
    "\n",
    "To find a metric, SageMaker searches the logs that your algorithm emits and finds logs that match the regular expression that you specify for that metric. \n",
    "\n",
    "Defining Training Metrics (SageMaker Python SDK)\n",
    "Define the metrics that you want to send to CloudWatch by specifying a list of metric names and regular expressions as the metric_definitions argument when you initialize an Estimator object. For example, if you want to monitor both the train:error and validation:error metrics in CloudWatch, your Estimator initialization would look like the following:\n",
    "```python\n",
    "Estimator(\n",
    "    ...,\n",
    "    sagemaker_session = sm_sess,\n",
    "    tags = [{'Key': 'my-experiments', 'Value': 'demo2'}])\n",
    "\n",
    "estimator.fit(\n",
    "    ...,\n",
    "    experiment_config = {\n",
    "        # \"ExperimentName\"\n",
    "        \"TrialName\" : demo_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\" : \"TrainingJob\",\n",
    "    })\n",
    "```\n",
    "\n",
    "In the regex for the train:error metric defined above, the first part of the regex finds the exact text `\"Train_error=\"`, and the expression `(.*?);` captures zero or more of any character until the first `;` semicolon character.\n",
    "\n",
    "For more information about training by using Amazon SageMaker Python SDK estimators, see https://github.com/aws/sagemaker-python-sdk#sagemaker-python-sdk-overview.\n",
    "\n",
    "# Scripts\n",
    "https://github.com/huggingface/transformers/tree/master/examples/text-classification\n",
    "\n",
    "\n",
    "## GLUE\n",
    "\n",
    "    export TASK_NAME=MRPC\n",
    "\n",
    "    python run_glue.py \\\n",
    "      --model_name_or_path bert-base-cased \\\n",
    "      --task_name $TASK_NAME \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --max_seq_length 128 \\\n",
    "      --per_device_train_batch_size 32 \\\n",
    "      --learning_rate 2e-5 \\\n",
    "      --num_train_epochs 3.0 \\\n",
    "      --output_dir /tmp/$TASK_NAME/\n",
    "      \n",
    "## XNLI      \n",
    "    export XNLI_DIR=/path/to/XNLI\n",
    "\n",
    "    python run_xnli.py \\\n",
    "      --model_name_or_path bert-base-multilingual-cased \\\n",
    "      --language de \\\n",
    "      --train_language en \\\n",
    "      --do_train \\\n",
    "      --do_eval \\\n",
    "      --data_dir $XNLI_DIR \\\n",
    "      --per_device_train_batch_size 32 \\\n",
    "      --learning_rate 5e-5 \\\n",
    "      --num_train_epochs 2.0 \\\n",
    "      --max_seq_length 128 \\\n",
    "      --output_dir /tmp/debug_xnli/ \\\n",
    "      --save_steps -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Estimator ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface.estimator import HuggingFace\n",
    "import json\n",
    "\n",
    "local_estimator = HuggingFace(entry_point='run_glue.py',\n",
    "                            source_dir='../../transformers/examples/text-classification',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='local',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version={'transformers':'4.1.1','datasets':'1.1.3'},\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {\n",
    "                                'model_name_or_path': 'distilbert-base-cased',\n",
    "                                'task_name':'MRPC',\n",
    "                                'do_train': True,\n",
    "                                'do_eval': True,\n",
    "                                'max_seq_length':'128',\n",
    "                                'per_device_train_batch_size':32,\n",
    "                                'learning_rate':2e-5,\n",
    "                                'num_train_epochs': 3.0\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sl8nyno36v-algo-1-3upme ... \n",
      "\u001b[1BAttaching to sl8nyno36v-algo-1-3upme2mdone\u001b[0m\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:11,627 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:11,629 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:11,645 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:11,648 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:11,678 botocore.credentials INFO     Found credentials in environment variables.\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:11,912 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m /opt/conda/bin/python -m pip install -r requirements.txt\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.1.3)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (3.14.0)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.4)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.0)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.25.1)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.0)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.49.0)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Collecting sentencepiece!=0.1.92\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m   Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |ââââââââââââââââââââââââââââââââ| 1.1 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.25.11)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 3)) (1.15.0)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.4)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Installing collected packages: sentencepiece\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Successfully installed sentencepiece-0.1.94\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:13,521 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:13,542 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:13,560 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m 2020-12-30 16:05:13,577 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Training Env:\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m {\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"channel_input_dirs\": {},\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"current_host\": \"algo-1-3upme\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"hosts\": [\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"algo-1-3upme\"\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     ],\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"model_name_or_path\": \"distilbert-base-cased\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"task_name\": \"MRPC\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"do_train\": true,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"do_eval\": true,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"max_seq_length\": \"128\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"per_device_train_batch_size\": 32,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"learning_rate\": 2e-05,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"num_train_epochs\": 3.0\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     },\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"input_data_config\": {},\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"job_name\": \"huggingface-sdk-extension-2020-12-30-16-05-09-057\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"master_hostname\": \"algo-1-3upme\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-30-16-05-09-057/source/sourcedir.tar.gz\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"module_name\": \"run_glue\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"current_host\": \"algo-1-3upme\",\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         \"hosts\": [\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m             \"algo-1-3upme\"\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m         ]\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     },\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m     \"user_entry_point\": \"run_glue.py\"\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m }\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Environment variables:\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HOSTS=[\"algo-1-3upme\"]\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HPS={\"do_eval\":true,\"do_train\":true,\"learning_rate\":2e-05,\"max_seq_length\":\"128\",\"model_name_or_path\":\"distilbert-base-cased\",\"num_train_epochs\":3.0,\"per_device_train_batch_size\":32,\"task_name\":\"MRPC\"}\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_USER_ENTRY_POINT=run_glue.py\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-3upme\",\"hosts\":[\"algo-1-3upme\"]}\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_INPUT_DATA_CONFIG={}\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_CHANNELS=[]\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_CURRENT_HOST=algo-1-3upme\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_MODULE_NAME=run_glue\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-30-16-05-09-057/source/sourcedir.tar.gz\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-3upme\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-3upme\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"learning_rate\":2e-05,\"max_seq_length\":\"128\",\"model_name_or_path\":\"distilbert-base-cased\",\"num_train_epochs\":3.0,\"per_device_train_batch_size\":32,\"task_name\":\"MRPC\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-sdk-extension-2020-12-30-16-05-09-057\",\"log_level\":20,\"master_hostname\":\"algo-1-3upme\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-30-16-05-09-057/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-3upme\",\"hosts\":[\"algo-1-3upme\"]},\"user_entry_point\":\"run_glue.py\"}\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--learning_rate\",\"2e-05\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"distilbert-base-cased\",\"--num_train_epochs\",\"3.0\",\"--per_device_train_batch_size\",\"32\",\"--task_name\",\"MRPC\"]\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_MODEL_NAME_OR_PATH=distilbert-base-cased\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_TASK_NAME=MRPC\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_DO_TRAIN=true\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_DO_EVAL=true\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_MAX_SEQ_LENGTH=128\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=32\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_LEARNING_RATE=2e-05\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m SM_HP_NUM_TRAIN_EPOCHS=3.0\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m /opt/conda/bin/python run_glue.py --do_eval True --do_train True --learning_rate 2e-05 --max_seq_length 128 --model_name_or_path distilbert-base-cased --num_train_epochs 3.0 --per_device_train_batch_size 32 --task_name MRPC\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n",
      "\u001b[36msl8nyno36v-algo-1-3upme |\u001b[0m Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-057a275548e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlocal_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \"\"\"\n\u001b[1;32m   1418\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         self.model_artifacts = self.container.train(\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         )\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;31m# _stream_output() doesn't have the command line. We will handle the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m         \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "local_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface.estimator import HuggingFace\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='run_glue.py',\n",
    "                            source_dir='../../transformers/examples/text-classification',\n",
    "                            sagemaker_session=sess,\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version={'transformers':'4.1.1','datasets':'1.1.3'},\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {\n",
    "                                'model_name_or_path': 'distilbert-base-cased',\n",
    "                                'task_name':'MRPC',\n",
    "                                'do_train': True,\n",
    "                                'do_eval': True,\n",
    "                                'max_seq_length':'128',\n",
    "                                'per_device_train_batch_size':32,\n",
    "                                'learning_rate':2e-5,\n",
    "                                'num_train_epochs': 3.0\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-31 08:22:11 Starting - Starting the training job...\n",
      "2020-12-31 08:22:34 Starting - Launching requested ML instancesProfilerReport-1609402930: InProgress\n",
      "......\n",
      "2020-12-31 08:23:35 Starting - Preparing the instances for training......\n",
      "2020-12-31 08:24:36 Downloading - Downloading input data\n",
      "2020-12-31 08:24:36 Training - Downloading the training image.....................\n",
      "2020-12-31 08:28:12 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-31 08:28:12,243 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-31 08:28:12,266 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-31 08:28:12,498 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-31 08:28:12,878 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 3)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.4)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece\u001b[0m\n",
      "\u001b[34mSuccessfully installed sentencepiece-0.1.94\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-31 08:28:15,036 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"task_name\": \"MRPC\",\n",
      "        \"do_train\": true,\n",
      "        \"num_train_epochs\": 3.0,\n",
      "        \"do_eval\": true,\n",
      "        \"max_seq_length\": \"128\",\n",
      "        \"per_device_train_batch_size\": 32,\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"model_name_or_path\": \"distilbert-base-cased\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-sdk-extension-2020-12-31-08-22-10-312\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-31-08-22-10-312/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"learning_rate\":2e-05,\"max_seq_length\":\"128\",\"model_name_or_path\":\"distilbert-base-cased\",\"num_train_epochs\":3.0,\"per_device_train_batch_size\":32,\"task_name\":\"MRPC\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-31-08-22-10-312/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"learning_rate\":2e-05,\"max_seq_length\":\"128\",\"model_name_or_path\":\"distilbert-base-cased\",\"num_train_epochs\":3.0,\"per_device_train_batch_size\":32,\"task_name\":\"MRPC\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-sdk-extension-2020-12-31-08-22-10-312\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-31-08-22-10-312/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--learning_rate\",\"2e-05\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"distilbert-base-cased\",\"--num_train_epochs\",\"3.0\",\"--per_device_train_batch_size\",\"32\",\"--task_name\",\"MRPC\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_TASK_NAME=MRPC\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3.0\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=distilbert-base-cased\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python run_glue.py --do_eval True --do_train True --learning_rate 2e-05 --max_seq_length 128 --model_name_or_path distilbert-base-cased --num_train_epochs 3.0 --per_device_train_batch_size 32 --task_name MRPC\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m['run_glue.py', '--do_eval', '--do_train', '--learning_rate', '2e-05', '--max_seq_length', '128', '--model_name_or_path', 'distilbert-base-cased', '--num_train_epochs', '3.0', '--per_device_train_batch_size', '32', '--task_name', 'MRPC', '--output_dir', '/opt/ml/output/data']\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:43.990 algo-1:31 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:43.991 algo-1:31 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:43.991 algo-1:31 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:43.991 algo-1:31 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:44.017 algo-1:31 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:44.017 algo-1:31 INFO hook.py:461] Hook is writing from the hook with pid: 31\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:45.513 algo-1:31 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:45.514 algo-1:31 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-31 08:28:45.523 algo-1:31 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:19 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/opt/ml/output/data', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec31_08-28-19_algo-1', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/opt/ml/output/data', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/8.68k [00:00<?, ?B/s]#015Downloading: 28.7kB [00:00, 16.1MB/s]                   \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/4.97k [00:00<?, ?B/s]#015Downloading: 28.7kB [00:00, 19.9MB/s]                   \u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#015Downloading: 6.22kB [00:00, 3.90MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#015Downloading: 19.7kB [00:00, 106kB/s]#015Downloading: 54.5kB [00:00, 122kB/s]#015Downloading: 124kB [00:00, 152kB/s] #015Downloading: 280kB [00:00, 201kB/s]#015Downloading: 576kB [00:00, 273kB/s]#015Downloading: 959kB [00:01, 369kB/s]#015Downloading: 1.05MB [00:01, 928kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#015Downloading: 19.4kB [00:00, 103kB/s]#015Downloading: 54.3kB [00:00, 119kB/s]#015Downloading: 124kB [00:00, 150kB/s] #015Downloading: 298kB [00:00, 200kB/s]#015Downloading: 441kB [00:00, 582kB/s]\u001b[0m\n",
      "\u001b[34m#0150 examples [00:00, ? examples/s]#0151705 examples [00:00, 17044.33 examples/s]#0153300 examples [00:00, 16698.53 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#015                                #015#0150 examples [00:00, ? examples/s]#015                                #01512/31/2020 08:28:28 - INFO - filelock -   Lock 139800303634584 acquired on /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1301] 2020-12-31 08:28:28,367 >> https://huggingface.co/distilbert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplyt9e_gw\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 411/411 [00:00<00:00, 496kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1305] 2020-12-31 08:28:28,649 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1308] 2020-12-31 08:28:28,649 >> creating metadata file for /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\u001b[0m\n",
      "\u001b[34m2020-12-31 08:29:30,381 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:28 - INFO - filelock -   Lock 139800303634584 released on /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a.lock\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:431] 2020-12-31 08:28:28,650 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:467] 2020-12-31 08:28:28,651 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:431] 2020-12-31 08:28:28,933 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:467] 2020-12-31 08:28:28,933 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:29 - INFO - filelock -   Lock 139797608840104 acquired on /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1301] 2020-12-31 08:28:29,217 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvm6yksc0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]#015Downloading:  17%|ââ        | 36.9k/213k [00:00<00:00, 212kB/s]#015Downloading:  94%|ââââââââââ| 201k/213k [00:00<00:00, 282kB/s] #015Downloading: 100%|ââââââââââ| 213k/213k [00:00<00:00, 604kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1305] 2020-12-31 08:28:29,855 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1308] 2020-12-31 08:28:29,855 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:29 - INFO - filelock -   Lock 139797608840104 released on /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:30 - INFO - filelock -   Lock 139797608841112 acquired on /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1301] 2020-12-31 08:28:30,143 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5vnay570\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]#015Downloading:   8%|â         | 36.9k/436k [00:00<00:01, 214kB/s]#015Downloading:  46%|âââââ     | 201k/436k [00:00<00:00, 284kB/s] #015Downloading: 100%|ââââââââââ| 436k/436k [00:00<00:00, 1.10MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1305] 2020-12-31 08:28:30,827 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1308] 2020-12-31 08:28:30,827 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:30 - INFO - filelock -   Lock 139797608841112 released on /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2020-12-31 08:28:30,827 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2020-12-31 08:28:30,827 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:31 - INFO - filelock -   Lock 139800303634584 acquired on /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1301] 2020-12-31 08:28:31,151 >> https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi2h8yubw\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/263M [00:00<?, ?B/s]#015Downloading:   2%|â         | 4.13M/263M [00:00<00:06, 41.3MB/s]#015Downloading:   3%|â         | 8.25M/263M [00:00<00:06, 41.2MB/s]#015Downloading:   5%|â         | 12.8M/263M [00:00<00:05, 42.4MB/s]#015Downloading:   7%|â         | 17.5M/263M [00:00<00:05, 43.8MB/s]#015Downloading:   9%|â         | 22.4M/263M [00:00<00:05, 45.2MB/s]#015Downloading:  10%|â         | 27.3M/263M [00:00<00:05, 46.2MB/s]#015Downloading:  12%|ââ        | 32.2M/263M [00:00<00:04, 47.2MB/s]#015Downloading:  14%|ââ        | 37.3M/263M [00:00<00:04, 48.1MB/s]#015Downloading:  16%|ââ        | 42.3M/263M [00:00<00:04, 48.7MB/s]#015Downloading:  18%|ââ        | 47.3M/263M [00:01<00:04, 49.1MB/s]#015Downloading:  20%|ââ        | 52.3M/263M [00:01<00:04, 49.4MB/s]#015Downloading:  22%|âââ       | 57.6M/263M [00:01<00:04, 50.4MB/s]#015Downloading:  24%|âââ       | 63.7M/263M [00:01<00:03, 53.3MB/s]#015Downloading:  27%|âââ       | 69.9M/263M [00:01<00:03, 55.6MB/s]#015Downloading:  29%|âââ       | 76.1M/263M [00:01<00:03, 57.3MB/s]#015Downloading:  31%|ââââ      | 82.3M/263M [00:01<00:03, 58.6MB/s]#015Downloading:  33%|ââââ      | 88.2M/263M [00:01<00:02, 58.6MB/s]#015Downloading:  36%|ââââ      | 94.5M/263M [00:01<00:02, 59.8MB/s]#015Downloading:  38%|ââââ      | 101M/263M [00:01<00:02, 60.7MB/s] #015Downloading:  41%|ââââ      | 107M/263M [00:02<00:02, 57.8MB/s]#015Downloading:  43%|âââââ     | 113M/263M [00:02<00:02, 55.2MB/s]#015Downloading:  45%|âââââ     | 118M/263M [00:02<00:02, 52.6MB/s]#015Downloading:  47%|âââââ     | 124M/263M [00:02<00:02, 51.7MB/s]#015Downloading:  49%|âââââ     | 129M/263M [00:02<00:02, 51.1MB/s]#015Downloading:  51%|âââââ     | 134M/263M [00:02<00:02, 50.8MB/s]#015Downloading:  53%|ââââââ    | 139M/263M [00:02<00:02, 50.7MB/s]#015Downloading:  55%|ââââââ    | 144M/263M [00:02<00:02, 49.6MB/s]#015Downloading:  57%|ââââââ    | 149M/263M [00:02<00:02, 49.7MB/s]#015Downloading:  59%|ââââââ    | 154M/263M [00:02<00:02, 49.9MB/s]#015Downloading:  60%|ââââââ    | 159M/263M [00:03<00:02, 49.9MB/s]#015Downloading:  62%|âââââââ   | 164M/263M [00:03<00:01, 49.6MB/s]#015Downloading:  64%|âââââââ   | 169M/263M [00:03<00:01, 49.7MB/s]#015Downloading:  66%|âââââââ   | 174M/263M [00:03<00:01, 49.8MB/s]#015Downloading:  68%|âââââââ   | 179M/263M [00:03<00:01, 49.9MB/s]#015Downloading:  70%|âââââââ   | 184M/263M [00:03<00:01, 49.9MB/s]#015Downloading:  72%|ââââââââ  | 189M/263M [00:03<00:01, 50.0MB/s]#015Downloading:  74%|ââââââââ  | 194M/263M [00:03<00:01, 50.0MB/s]#015Downloading:  76%|ââââââââ  | 199M/263M [00:03<00:01, 50.1MB/s]#015Downloading:  78%|ââââââââ  | 205M/263M [00:03<00:01, 51.3MB/s]#015Downloading:  80%|ââââââââ  | 211M/263M [00:04<00:00, 53.9MB/s]#015Downloading:  82%|âââââââââ | 217M/263M [00:04<00:00, 56.1MB/s]#015Downloading:  85%|âââââââââ | 223M/263M [00:04<00:00, 57.3MB/s]#015Downloading:  87%|âââââââââ | 229M/263M [00:04<00:00, 58.6MB/s]#015Downloading:  89%|âââââââââ | 235M/263M [00:04<00:00, 59.7MB/s]#015Downloading:  92%|ââââââââââ| 241M/263M [00:04<00:00, 58.4MB/s]#015Downloading:  94%|ââââââââââ| 247M/263M [00:04<00:00, 52.6MB/s]#015Downloading:  96%|ââââââââââ| 253M/263M [00:04<00:00, 51.7MB/s]#015Downloading:  98%|ââââââââââ| 258M/263M [00:04<00:00, 50.8MB/s]#015Downloading: 100%|ââââââââââ| 263M/263M [00:05<00:00, 50.9MB/s]#015Downloading: 100%|ââââââââââ| 263M/263M [00:05<00:00, 52.2MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1305] 2020-12-31 08:28:36,253 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1308] 2020-12-31 08:28:36,253 >> creating metadata file for /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:36 - INFO - filelock -   Lock 139800303634584 released on /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19.lock\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1024] 2020-12-31 08:28:36,253 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1132] 2020-12-31 08:28:38,515 >> Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1143] 2020-12-31 08:28:38,515 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/4 [00:00<?, ?ba/s]#015 25%|âââ       | 1/4 [00:00<00:00,  9.17ba/s]#015 75%|ââââââââ  | 3/4 [00:00<00:00, 10.17ba/s]#015100%|ââââââââââ| 4/4 [00:00<00:00, 13.12ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?ba/s]#015100%|ââââââââââ| 1/1 [00:00<00:00, 29.95ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?ba/s]#015100%|ââââââââââ| 2/2 [00:00<00:00, 14.81ba/s]#015100%|ââââââââââ| 2/2 [00:00<00:00, 14.77ba/s]\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:39 - INFO - __main__ -   Sample 2619 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 2916, 'input_ids': [101, 1109, 10830, 1127, 1678, 1146, 1114, 24987, 1149, 13260, 1147, 1692, 1222, 7277, 2180, 5303, 117, 3455, 3081, 5097, 1104, 4961, 1149, 13260, 9966, 1222, 1140, 119, 102, 20661, 1127, 1678, 1146, 1114, 24987, 1149, 13260, 1147, 1692, 1222, 7277, 2180, 5303, 117, 3455, 170, 3081, 118, 3674, 21100, 2998, 1106, 1103, 2175, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .'}.\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:39 - INFO - __main__ -   Sample 456 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 509, 'input_ids': [101, 20394, 11252, 1424, 3878, 1684, 1111, 1103, 4116, 118, 5534, 1433, 1132, 170, 6539, 4010, 1111, 9283, 1105, 6646, 1110, 1919, 1344, 3075, 1104, 1397, 3625, 112, 188, 5200, 1728, 1107, 1594, 118, 7820, 20394, 11252, 15449, 119, 102, 9018, 1116, 1107, 20394, 11252, 15449, 112, 188, 4116, 118, 5534, 1433, 1132, 170, 6539, 4010, 1111, 9283, 117, 1105, 6646, 1110, 1919, 1344, 3075, 1104, 3625, 112, 188, 5200, 1728, 1107, 1103, 1594, 118, 187, 15677, 3660, 1805, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\"}.\u001b[0m\n",
      "\u001b[34m12/31/2020 08:28:39 - INFO - __main__ -   Sample 102 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 116, 'input_ids': [101, 6433, 111, 11767, 112, 188, 2260, 4482, 7448, 2174, 1116, 5799, 125, 119, 1969, 1827, 1106, 5103, 1495, 119, 1851, 117, 1229, 11896, 1116, 1810, 4426, 2174, 1116, 2204, 127, 119, 126, 1827, 1106, 122, 117, 20278, 119, 1851, 119, 102, 1109, 6433, 111, 11767, 112, 188, 2260, 10146, 1108, 1146, 122, 119, 3453, 1827, 117, 1137, 121, 119, 1407, 3029, 117, 1106, 5311, 1559, 119, 5599, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\"}.\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.67k [00:00<?, ?B/s]#015Downloading: 4.39kB [00:00, 3.86MB/s]                   \u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:388] 2020-12-31 08:28:43,678 >> The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:388] 2020-12-31 08:28:43,678 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2020-12-31 08:28:43,680 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:704] 2020-12-31 08:28:43,680 >>   Num examples = 3668\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:705] 2020-12-31 08:28:43,680 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:706] 2020-12-31 08:28:43,680 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:707] 2020-12-31 08:28:43,680 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:708] 2020-12-31 08:28:43,680 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:709] 2020-12-31 08:28:43,680 >>   Total optimization steps = 345\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/345 [00:00<?, ?it/s]#015  0%|          | 1/345 [00:02<11:36,  2.03s/it]#015  1%|          | 2/345 [00:02<08:19,  1.46s/it]#015  1%|          | 3/345 [00:02<06:01,  1.06s/it]#015  1%|          | 4/345 [00:02<04:24,  1.29it/s]#015  1%|â         | 5/345 [00:02<03:17,  1.72it/s]#015  2%|â         | 6/345 [00:02<02:30,  2.26it/s]#015  2%|â         | 7/345 [00:02<01:57,  2.88it/s]#015  2%|â         | 8/345 [00:02<01:34,  3.57it/s]#015  3%|â         | 9/345 [00:03<01:18,  4.29it/s]#015  3%|â         | 10/345 [00:03<01:07,  4.99it/s]#015  3%|â         | 11/345 [00:03<00:59,  5.64it/s]#015  3%|â         | 12/345 [00:03<00:53,  6.22it/s]#015  4%|â         | 13/345 [00:03<00:49,  6.71it/s]#015  4%|â         | 14/345 [00:03<00:46,  7.09it/s]#015  4%|â         | 15/345 [00:03<00:44,  7.40it/s]#015  5%|â         | 16/345 [00:03<00:43,  7.57it/s]#015  5%|â         | 17/345 [00:03<00:42,  7.75it/s]#015  5%|â         | 18/345 [00:04<00:41,  7.85it/s]#015  6%|â         | 19/345 [00:04<00:40,  7.96it/s]#015  6%|â         | 20/345 [00:04<00:40,  8.02it/s]#015  6%|â         | 21/345 [00:04<00:40,  8.07it/s]#015  6%|â         | 22/345 [00:04<00:40,  8.03it/s]#015  7%|â         | 23/345 [00:04<00:40,  8.04it/s]#015  7%|â         | 24/345 [00:04<00:39,  8.07it/s]#015  7%|â         | 25/345 [00:04<00:39,  8.07it/s]#015  8%|â         | 26/345 [00:05<00:39,  8.11it/s]#015  8%|â         | 27/345 [00:05<00:39,  8.11it/s]#015  8%|â         | 28/345 [00:05<00:38,  8.14it/s]#015  8%|â         | 29/345 [00:05<00:39,  8.10it/s]#015  9%|â         | 30/345 [00:05<00:39,  8.06it/s]#015  9%|â         | 31/345 [00:05<00:38,  8.10it/s]#015  9%|â         | 32/345 [00:05<00:38,  8.13it/s]#015 10%|â         | 33/345 [00:05<00:38,  8.12it/s]#015 10%|â         | 34/345 [00:06<00:38,  8.14it/s]#015 10%|â         | 35/345 [00:06<00:38,  8.12it/s]#015 10%|â         | 36/345 [00:06<00:38,  8.10it/s]#015 11%|â         | 37/345 [00:06<00:38,  8.10it/s]#015 11%|â         | 38/345 [00:06<00:37,  8.13it/s]#015 11%|ââ        | 39/345 [00:06<00:37,  8.10it/s]#015 12%|ââ        | 40/345 [00:06<00:37,  8.08it/s]#015 12%|ââ        | 41/345 [00:06<00:37,  8.09it/s]#015 12%|ââ        | 42/345 [00:07<00:37,  8.08it/s]#015 12%|ââ        | 43/345 [00:07<00:37,  8.09it/s]#015 13%|ââ        | 44/345 [00:07<00:37,  8.09it/s]#015 13%|ââ        | 45/345 [00:07<00:37,  8.09it/s]#015 13%|ââ        | 46/345 [00:07<00:37,  8.08it/s]#015 14%|ââ        | 47/345 [00:07<00:36,  8.08it/s]#015 14%|ââ        | 48/345 [00:07<00:36,  8.08it/s]#015 14%|ââ        | 49/345 [00:07<00:36,  8.08it/s]#015 14%|ââ        | 50/345 [00:08<00:36,  8.07it/s]#015 15%|ââ        | 51/345 [00:08<00:36,  8.08it/s]#015 15%|ââ        | 52/345 [00:08<00:36,  8.09it/s]#015 15%|ââ        | 53/345 [00:08<00:36,  8.10it/s]#015 16%|ââ        | 54/345 [00:08<00:35,  8.10it/s]#015 16%|ââ        | 55/345 [00:08<00:35,  8.09it/s]#015 16%|ââ        | 56/345 [00:08<00:35,  8.09it/s]#015 17%|ââ        | 57/345 [00:08<00:35,  8.08it/s]#015 17%|ââ        | 58/345 [00:09<00:35,  8.08it/s]#015 17%|ââ        | 59/345 [00:09<00:35,  8.02it/s]#015 17%|ââ        | 60/345 [00:09<00:35,  8.04it/s]#015 18%|ââ        | 61/345 [00:09<00:35,  7.95it/s]#015 18%|ââ        | 62/345 [00:09<00:35,  7.93it/s]#015 18%|ââ        | 63/345 [00:09<00:35,  7.97it/s]#015 19%|ââ        | 64/345 [00:09<00:35,  8.00it/s]#015 19%|ââ        | 65/345 [00:09<00:35,  7.99it/s]#015 19%|ââ        | 66/345 [00:10<00:34,  8.02it/s]#015 19%|ââ        | 67/345 [00:10<00:34,  8.04it/s]#015 20%|ââ        | 68/345 [00:10<00:34,  8.06it/s]#015 20%|ââ        | 69/345 [00:10<00:34,  8.08it/s]#015 20%|ââ        | 70/345 [00:10<00:34,  8.08it/s]#015 21%|ââ        | 71/345 [00:10<00:33,  8.07it/s]#015 21%|ââ        | 72/345 [00:10<00:33,  8.07it/s]#015 21%|ââ        | 73/345 [00:10<00:33,  8.03it/s]#015 21%|âââ       | 74/345 [00:11<00:33,  8.01it/s]#015 22%|âââ       | 75/345 [00:11<00:33,  8.03it/s]#015 22%|âââ       | 76/345 [00:11<00:33,  8.04it/s]#015 22%|âââ       | 77/345 [00:11<00:33,  8.05it/s]#015 23%|âââ       | 78/345 [00:11<00:33,  8.06it/s]#015 23%|âââ       | 79/345 [00:11<00:33,  8.06it/s]#015 23%|âââ       | 80/345 [00:11<00:32,  8.07it/s]#015 23%|âââ       | 81/345 [00:11<00:32,  8.07it/s]#015 24%|âââ       | 82/345 [00:12<00:32,  8.07it/s]#015 24%|âââ       | 83/345 [00:12<00:32,  8.08it/s]#015 24%|âââ       | 84/345 [00:12<00:32,  8.08it/s]#015 25%|âââ       | 85/345 [00:12<00:32,  8.09it/s]#015 25%|âââ       | 86/345 [00:12<00:32,  8.08it/s]#015 25%|âââ       | 87/345 [00:12<00:31,  8.08it/s]#015 26%|âââ       | 88/345 [00:12<00:31,  8.08it/s]#015 26%|âââ       | 89/345 [00:12<00:31,  8.10it/s]#015 26%|âââ       | 90/345 [00:13<00:31,  8.09it/s]#015 26%|âââ       | 91/345 [00:13<00:31,  8.08it/s]#015 27%|âââ       | 92/345 [00:13<00:31,  8.08it/s]#015 27%|âââ       | 93/345 [00:13<00:31,  8.08it/s]#015 27%|âââ       | 94/345 [00:13<00:31,  8.08it/s]#015 28%|âââ       | 95/345 [00:13<00:30,  8.09it/s]#015 28%|âââ       | 96/345 [00:13<00:30,  8.08it/s]#015 28%|âââ       | 97/345 [00:13<00:30,  8.09it/s]#015 28%|âââ       | 98/345 [00:14<00:30,  8.09it/s]#015 29%|âââ       | 99/345 [00:14<00:30,  8.08it/s]#015 29%|âââ       | 100/345 [00:14<00:30,  8.08it/s]#015 29%|âââ       | 101/345 [00:14<00:30,  8.09it/s]#015 30%|âââ       | 102/345 [00:14<00:30,  7.97it/s]#015 30%|âââ       | 103/345 [00:14<00:30,  7.98it/s]#015 30%|âââ       | 104/345 [00:14<00:30,  7.99it/s]#015 30%|âââ       | 105/345 [00:14<00:30,  7.99it/s]#015 31%|âââ       | 106/345 [00:15<00:29,  7.99it/s]#015 31%|âââ       | 107/345 [00:15<00:29,  8.00it/s]#015 31%|ââââ      | 108/345 [00:15<00:29,  8.01it/s]#015 32%|ââââ      | 109/345 [00:15<00:29,  8.02it/s]#015 32%|ââââ      | 110/345 [00:15<00:29,  8.01it/s]#015 32%|ââââ      | 111/345 [00:15<00:29,  8.00it/s]#015 32%|ââââ      | 112/345 [00:15<00:29,  8.00it/s]#015 33%|ââââ      | 113/345 [00:15<00:28,  8.00it/s]#015 33%|ââââ      | 114/345 [00:16<00:28,  8.00it/s]#015 34%|ââââ      | 116/345 [00:16<00:27,  8.39it/s]#015 34%|ââââ      | 117/345 [00:16<00:27,  8.30it/s]#015 34%|ââââ      | 118/345 [00:16<00:27,  8.24it/s]#015 34%|ââââ      | 119/345 [00:16<00:27,  8.19it/s]#015 35%|ââââ      | 120/345 [00:16<00:27,  8.12it/s]#015 35%|ââââ      | 121/345 [00:16<00:27,  8.11it/s]#015 35%|ââââ      | 122/345 [00:16<00:27,  8.10it/s]#015 36%|ââââ      | 123/345 [00:17<00:27,  8.09it/s]#015 36%|ââââ      | 124/345 [00:17<00:27,  8.09it/s]#015 36%|ââââ      | 125/345 [00:17<00:27,  8.09it/s]#015 37%|ââââ      | 126/345 [00:17<00:27,  8.10it/s]#015 37%|ââââ      | 127/345 [00:17<00:26,  8.09it/s]#015 37%|ââââ      | 128/345 [00:17<00:26,  8.04it/s]#015 37%|ââââ      | 129/345 [00:17<00:26,  8.04it/s]#015 38%|ââââ      | 130/345 [00:17<00:26,  8.05it/s]#015 38%|ââââ      | 131/345 [00:18<00:26,  8.06it/s]#015 38%|ââââ      | 132/345 [00:18<00:26,  8.06it/s]#015 39%|ââââ      | 133/345 [00:18<00:26,  8.06it/s]#015 39%|ââââ      | 134/345 [00:18<00:26,  8.06it/s]#015 39%|ââââ      | 135/345 [00:18<00:26,  8.06it/s]#015 39%|ââââ      | 136/345 [00:18<00:25,  8.07it/s]#015 40%|ââââ      | 137/345 [00:18<00:25,  8.06it/s]#015 40%|ââââ      | 138/345 [00:18<00:25,  8.06it/s]#015 40%|ââââ      | 139/345 [00:19<00:25,  8.05it/s]#015 41%|ââââ      | 140/345 [00:19<00:25,  8.07it/s]#015 41%|ââââ      | 141/345 [00:19<00:25,  8.08it/s]#015 41%|ââââ      | 142/345 [00:19<00:25,  8.09it/s]#015 41%|âââââ     | 143/345 [00:19<00:24,  8.09it/s]#015 42%|âââââ     | 144/345 [00:19<00:24,  8.10it/s]#015 42%|âââââ     | 145/345 [00:19<00:24,  8.10it/s]#015 42%|âââââ     | 146/345 [00:19<00:24,  8.10it/s]#015 43%|âââââ     | 147/345 [00:20<00:24,  8.10it/s]#015 43%|âââââ     | 148/345 [00:20<00:24,  8.11it/s]#015 43%|âââââ     | 149/345 [00:20<00:24,  8.12it/s]#015 43%|âââââ     | 150/345 [00:20<00:24,  8.12it/s]#015 44%|âââââ     | 151/345 [00:20<00:23,  8.12it/s]#015 44%|âââââ     | 152/345 [00:20<00:23,  8.13it/s]#015 44%|âââââ     | 153/345 [00:20<00:23,  8.11it/s]#015 45%|âââââ     | 154/345 [00:20<00:23,  8.11it/s]#015 45%|âââââ     | 155/345 [00:21<00:23,  8.03it/s]#015 45%|âââââ     | 156/345 [00:21<00:23,  8.05it/s]#015 46%|âââââ     | 157/345 [00:21<00:23,  8.07it/s]#015 46%|âââââ     | 158/345 [00:21<00:23,  8.08it/s]#015 46%|âââââ     | 159/345 [00:21<00:22,  8.09it/s]#015 46%|âââââ     | 160/345 [00:21<00:22,  8.10it/s]#015 47%|âââââ     | 161/345 [00:21<00:22,  8.11it/s]#015 47%|âââââ     | 162/345 [00:21<00:22,  8.10it/s]#015 47%|âââââ     | 163/345 [00:22<00:22,  7.95it/s]#015 48%|âââââ     | 164/345 [00:22<00:23,  7.75it/s]#015 48%|âââââ     | 165/345 [00:22<00:23,  7.68it/s]#015 48%|âââââ     | 166/345 [00:22<00:23,  7.74it/s]#015 48%|âââââ     | 167/345 [00:22<00:22,  7.81it/s]#015 49%|âââââ     | 168/345 [00:22<00:22,  7.86it/s]#015 49%|âââââ     | 169/345 [00:22<00:22,  7.89it/s]#015 49%|âââââ     | 170/345 [00:22<00:22,  7.93it/s]#015 50%|âââââ     | 171/345 [00:23<00:21,  7.93it/s]#015 50%|âââââ     | 172/345 [00:23<00:21,  7.98it/s]#015 50%|âââââ     | 173/345 [00:23<00:21,  8.03it/s]#015 50%|âââââ     | 174/345 [00:23<00:21,  8.05it/s]#015 51%|âââââ     | 175/345 [00:23<00:21,  8.08it/s]#015 51%|âââââ     | 176/345 [00:23<00:20,  8.09it/s]#015 51%|ââââââ    | 177/345 [00:23<00:20,  8.10it/s]#015 52%|ââââââ    | 178/345 [00:23<00:20,  8.10it/s]#015 52%|ââââââ    | 179/345 [00:24<00:20,  8.09it/s]#015 52%|ââââââ    | 180/345 [00:24<00:20,  8.10it/s]#015 52%|ââââââ    | 181/345 [00:24<00:20,  8.10it/s]#015 53%|ââââââ    | 182/345 [00:24<00:20,  8.09it/s]#015 53%|ââââââ    | 183/345 [00:24<00:20,  8.07it/s]#015 53%|ââââââ    | 184/345 [00:24<00:19,  8.07it/s]#015 54%|ââââââ    | 185/345 [00:24<00:19,  8.07it/s]#015 54%|ââââââ    | 186/345 [00:24<00:19,  8.07it/s]#015 54%|ââââââ    | 187/345 [00:25<00:19,  8.07it/s]#015 54%|ââââââ    | 188/345 [00:25<00:19,  8.07it/s]#015 55%|ââââââ    | 189/345 [00:25<00:19,  8.07it/s]#015 55%|ââââââ    | 190/345 [00:25<00:19,  8.06it/s]#015 55%|ââââââ    | 191/345 [00:25<00:19,  8.07it/s]#015 56%|ââââââ    | 192/345 [00:25<00:18,  8.07it/s]#015 56%|ââââââ    | 193/345 [00:25<00:18,  8.07it/s]#015 56%|ââââââ    | 194/345 [00:25<00:18,  8.07it/s]#015 57%|ââââââ    | 195/345 [00:26<00:18,  8.07it/s]#015 57%|ââââââ    | 196/345 [00:26<00:18,  8.07it/s]#015 57%|ââââââ    | 197/345 [00:26<00:18,  8.07it/s]#015 57%|ââââââ    | 198/345 [00:26<00:18,  8.06it/s]#015 58%|ââââââ    | 199/345 [00:26<00:18,  8.06it/s]#015 58%|ââââââ    | 200/345 [00:26<00:17,  8.07it/s]#015 58%|ââââââ    | 201/345 [00:26<00:17,  8.08it/s]#015 59%|ââââââ    | 202/345 [00:26<00:17,  8.08it/s]#015 59%|ââââââ    | 203/345 [00:27<00:17,  8.07it/s]#015 59%|ââââââ    | 204/345 [00:27<00:17,  8.06it/s]#015 59%|ââââââ    | 205/345 [00:27<00:17,  8.07it/s]#015 60%|ââââââ    | 206/345 [00:27<00:17,  8.06it/s]#015 60%|ââââââ    | 207/345 [00:27<00:17,  8.05it/s]#015 60%|ââââââ    | 208/345 [00:27<00:17,  8.06it/s]#015 61%|ââââââ    | 209/345 [00:27<00:16,  8.06it/s]#015 61%|ââââââ    | 210/345 [00:27<00:16,  8.06it/s]#015 61%|ââââââ    | 211/345 [00:28<00:16,  8.06it/s]#015 61%|âââââââ   | 212/345 [00:28<00:16,  8.05it/s]#015 62%|âââââââ   | 213/345 [00:28<00:16,  8.06it/s]#015 62%|âââââââ   | 214/345 [00:28<00:16,  8.06it/s]#015 62%|âââââââ   | 215/345 [00:28<00:16,  8.07it/s]#015 63%|âââââââ   | 216/345 [00:28<00:15,  8.07it/s]#015 63%|âââââââ   | 217/345 [00:28<00:15,  8.07it/s]#015 63%|âââââââ   | 218/345 [00:28<00:15,  8.07it/s]#015 63%|âââââââ   | 219/345 [00:29<00:15,  8.08it/s]#015 64%|âââââââ   | 220/345 [00:29<00:15,  8.01it/s]#015 64%|âââââââ   | 221/345 [00:29<00:15,  8.02it/s]#015 64%|âââââââ   | 222/345 [00:29<00:15,  8.04it/s]#015 65%|âââââââ   | 223/345 [00:29<00:15,  8.04it/s]#015 65%|âââââââ   | 224/345 [00:29<00:15,  8.05it/s]#015 65%|âââââââ   | 225/345 [00:29<00:14,  8.05it/s]#015 66%|âââââââ   | 226/345 [00:29<00:14,  8.04it/s]#015 66%|âââââââ   | 227/345 [00:30<00:14,  8.04it/s]#015 66%|âââââââ   | 228/345 [00:30<00:14,  8.03it/s]#015 66%|âââââââ   | 229/345 [00:30<00:14,  7.98it/s]#015 67%|âââââââ   | 231/345 [00:30<00:13,  8.38it/s]#015 67%|âââââââ   | 232/345 [00:30<00:13,  8.27it/s]#015 68%|âââââââ   | 233/345 [00:30<00:13,  8.20it/s]#015 68%|âââââââ   | 234/345 [00:30<00:13,  8.15it/s]#015 68%|âââââââ   | 235/345 [00:30<00:13,  8.11it/s]#015 68%|âââââââ   | 236/345 [00:31<00:13,  8.09it/s]#015 69%|âââââââ   | 237/345 [00:31<00:13,  8.07it/s]#015 69%|âââââââ   | 238/345 [00:31<00:13,  8.07it/s]#015 69%|âââââââ   | 239/345 [00:31<00:13,  8.06it/s]#015 70%|âââââââ   | 240/345 [00:31<00:13,  8.05it/s]#015 70%|âââââââ   | 241/345 [00:31<00:12,  8.06it/s]#015 70%|âââââââ   | 242/345 [00:31<00:12,  8.05it/s]#015 70%|âââââââ   | 243/345 [00:31<00:12,  8.05it/s]#015 71%|âââââââ   | 244/345 [00:32<00:12,  8.05it/s]#015 71%|âââââââ   | 245/345 [00:32<00:12,  8.05it/s]#015 71%|ââââââââ  | 246/345 [00:32<00:12,  8.04it/s]#015 72%|ââââââââ  | 247/345 [00:32<00:12,  8.04it/s]#015 72%|ââââââââ  | 248/345 [00:32<00:12,  8.04it/s]#015 72%|ââââââââ  | 249/345 [00:32<00:11,  8.04it/s]#015 72%|ââââââââ  | 250/345 [00:32<00:11,  8.03it/s]#015 73%|ââââââââ  | 251/345 [00:32<00:11,  8.04it/s]#015 73%|ââââââââ  | 252/345 [00:33<00:11,  8.04it/s]#015 73%|ââââââââ  | 253/345 [00:33<00:11,  8.05it/s]#015 74%|ââââââââ  | 254/345 [00:33<00:11,  8.05it/s]#015 74%|ââââââââ  | 255/345 [00:33<00:11,  8.05it/s]#015 74%|ââââââââ  | 256/345 [00:33<00:11,  8.05it/s]#015 74%|ââââââââ  | 257/345 [00:33<00:10,  8.05it/s]#015 75%|ââââââââ  | 258/345 [00:33<00:10,  8.05it/s]#015 75%|ââââââââ  | 259/345 [00:33<00:10,  8.04it/s]#015 75%|ââââââââ  | 260/345 [00:34<00:10,  8.04it/s]#015 76%|ââââââââ  | 261/345 [00:34<00:10,  7.98it/s]#015 76%|ââââââââ  | 262/345 [00:34<00:10,  7.99it/s]#015 76%|ââââââââ  | 263/345 [00:34<00:10,  8.00it/s]#015 77%|ââââââââ  | 264/345 [00:34<00:10,  8.01it/s]#015 77%|ââââââââ  | 265/345 [00:34<00:10,  7.91it/s]#015 77%|ââââââââ  | 266/345 [00:34<00:10,  7.88it/s]#015 77%|ââââââââ  | 267/345 [00:34<00:09,  7.94it/s]#015 78%|ââââââââ  | 268/345 [00:35<00:09,  7.99it/s]#015 78%|ââââââââ  | 269/345 [00:35<00:09,  7.96it/s]#015 78%|ââââââââ  | 270/345 [00:35<00:09,  8.00it/s]#015 79%|ââââââââ  | 271/345 [00:35<00:09,  8.02it/s]#015 79%|ââââââââ  | 272/345 [00:35<00:09,  8.03it/s]#015 79%|ââââââââ  | 273/345 [00:35<00:08,  8.05it/s]#015 79%|ââââââââ  | 274/345 [00:35<00:08,  8.07it/s]#015 80%|ââââââââ  | 275/345 [00:35<00:08,  8.09it/s]#015 80%|ââââââââ  | 276/345 [00:36<00:08,  8.11it/s]#015 80%|ââââââââ  | 277/345 [00:36<00:08,  8.11it/s]#015 81%|ââââââââ  | 278/345 [00:36<00:08,  8.09it/s]#015 81%|ââââââââ  | 279/345 [00:36<00:08,  8.10it/s]#015 81%|ââââââââ  | 280/345 [00:36<00:08,  8.09it/s]#015 81%|âââââââââ | 281/345 [00:36<00:07,  8.09it/s]#015 82%|ââââ\u001b[0m\n",
      "\u001b[34mâââââ | 282/345 [00:36<00:07,  8.09it/s]#015 82%|âââââââââ | 283/345 [00:36<00:07,  8.10it/s]#015 82%|âââââââââ | 284/345 [00:37<00:07,  8.11it/s]#015 83%|âââââââââ | 285/345 [00:37<00:07,  8.11it/s]#015 83%|âââââââââ | 286/345 [00:37<00:07,  8.11it/s]#015 83%|âââââââââ | 287/345 [00:37<00:07,  8.12it/s]#015 83%|âââââââââ | 288/345 [00:37<00:07,  8.11it/s]#015 84%|âââââââââ | 289/345 [00:37<00:06,  8.11it/s]#015 84%|âââââââââ | 290/345 [00:37<00:06,  8.12it/s]#015 84%|âââââââââ | 291/345 [00:37<00:06,  8.11it/s]#015 85%|âââââââââ | 292/345 [00:38<00:06,  8.11it/s]#015 85%|âââââââââ | 293/345 [00:38<00:06,  8.12it/s]#015 85%|âââââââââ | 294/345 [00:38<00:06,  8.10it/s]#015 86%|âââââââââ | 295/345 [00:38<00:06,  8.10it/s]#015 86%|âââââââââ | 296/345 [00:38<00:06,  8.10it/s]#015 86%|âââââââââ | 297/345 [00:38<00:05,  8.11it/s]#015 86%|âââââââââ | 298/345 [00:38<00:05,  8.12it/s]#015 87%|âââââââââ | 299/345 [00:38<00:05,  8.11it/s]#015 87%|âââââââââ | 300/345 [00:39<00:05,  8.11it/s]#015 87%|âââââââââ | 301/345 [00:39<00:05,  8.11it/s]#015 88%|âââââââââ | 302/345 [00:39<00:05,  8.09it/s]#015 88%|âââââââââ | 303/345 [00:39<00:05,  7.98it/s]#015 88%|âââââââââ | 304/345 [00:39<00:05,  8.01it/s]#015 88%|âââââââââ | 305/345 [00:39<00:04,  8.04it/s]#015 89%|âââââââââ | 306/345 [00:39<00:04,  7.92it/s]#015 89%|âââââââââ | 307/345 [00:39<00:04,  7.97it/s]#015 89%|âââââââââ | 308/345 [00:40<00:04,  8.00it/s]#015 90%|âââââââââ | 309/345 [00:40<00:04,  8.03it/s]#015 90%|âââââââââ | 310/345 [00:40<00:04,  8.04it/s]#015 90%|âââââââââ | 311/345 [00:40<00:04,  8.05it/s]#015 90%|âââââââââ | 312/345 [00:40<00:04,  8.05it/s]#015 91%|âââââââââ | 313/345 [00:40<00:04,  7.98it/s]#015 91%|âââââââââ | 314/345 [00:40<00:03,  8.01it/s]#015 91%|ââââââââââ| 315/345 [00:40<00:03,  8.02it/s]#015 92%|ââââââââââ| 316/345 [00:41<00:03,  8.04it/s]#015 92%|ââââââââââ| 317/345 [00:41<00:03,  8.05it/s]#015 92%|ââââââââââ| 318/345 [00:41<00:03,  8.00it/s]#015 92%|ââââââââââ| 319/345 [00:41<00:03,  8.03it/s]#015 93%|ââââââââââ| 320/345 [00:41<00:03,  8.04it/s]#015 93%|ââââââââââ| 321/345 [00:41<00:02,  8.06it/s]#015 93%|ââââââââââ| 322/345 [00:41<00:02,  8.07it/s]#015 94%|ââââââââââ| 323/345 [00:41<00:02,  8.05it/s]#015 94%|ââââââââââ| 324/345 [00:42<00:02,  8.06it/s]#015 94%|ââââââââââ| 325/345 [00:42<00:02,  8.08it/s]#015 94%|ââââââââââ| 326/345 [00:42<00:02,  8.07it/s]#015 95%|ââââââââââ| 327/345 [00:42<00:02,  8.03it/s]#015 95%|ââââââââââ| 328/345 [00:42<00:02,  8.05it/s]#015 95%|ââââââââââ| 329/345 [00:42<00:01,  8.07it/s]#015 96%|ââââââââââ| 330/345 [00:42<00:01,  8.09it/s]#015 96%|ââââââââââ| 331/345 [00:42<00:01,  8.09it/s]#015 96%|ââââââââââ| 332/345 [00:43<00:01,  8.09it/s]#015 97%|ââââââââââ| 333/345 [00:43<00:01,  8.09it/s]#015 97%|ââââââââââ| 334/345 [00:43<00:01,  8.10it/s]#015 97%|ââââââââââ| 335/345 [00:43<00:01,  8.05it/s]#015 97%|ââââââââââ| 336/345 [00:43<00:01,  8.03it/s]#015 98%|ââââââââââ| 337/345 [00:43<00:00,  8.03it/s]#015 98%|ââââââââââ| 338/345 [00:43<00:00,  8.04it/s]#015 98%|ââââââââââ| 339/345 [00:43<00:00,  8.04it/s]#015 99%|ââââââââââ| 340/345 [00:44<00:00,  8.04it/s]#015 99%|ââââââââââ| 341/345 [00:44<00:00,  8.04it/s]#015 99%|ââââââââââ| 342/345 [00:44<00:00,  8.02it/s]#015 99%|ââââââââââ| 343/345 [00:44<00:00,  8.01it/s]#015100%|ââââââââââ| 344/345 [00:44<00:00,  8.01it/s][INFO|trainer.py:862] 2020-12-31 08:29:28,297 >> \n",
      "\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015                                                 #015#015100%|ââââââââââ| 345/345 [00:44<00:00,  8.01it/s]#015100%|ââââââââââ| 345/345 [00:44<00:00,  7.73it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1226] 2020-12-31 08:29:28,298 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:289] 2020-12-31 08:29:28,300 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:814] 2020-12-31 08:29:28,950 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:28 - INFO - __main__ -   ***** Train results *****\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:28 - INFO - __main__ -     global_step = 345\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:28 - INFO - __main__ -     training_loss = 0.4789575106855752\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:28 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:388] 2020-12-31 08:29:28,986 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1412] 2020-12-31 08:29:28,987 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1413] 2020-12-31 08:29:28,987 >>   Num examples = 408\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1414] 2020-12-31 08:29:28,987 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/51 [00:00<?, ?it/s]#015 18%|ââ        | 9/51 [00:00<00:00, 80.14it/s]#015 33%|ââââ      | 17/51 [00:00<00:00, 77.98it/s]#015 49%|âââââ     | 25/51 [00:00<00:00, 76.58it/s]#015 65%|âââââââ   | 33/51 [00:00<00:00, 75.53it/s]#015 80%|ââââââââ  | 41/51 [00:00<00:00, 74.76it/s]#015 96%|ââââââââââ| 49/51 [00:00<00:00, 74.40it/s]12/31/2020 08:29:29 - INFO - /opt/conda/lib/python3.6/site-packages/datasets/metric.py -   Removing /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m#015100%|ââââââââââ| 51/51 [00:00<00:00, 72.39it/s]\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:29 - INFO - __main__ -   ***** Eval results mrpc *****\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:29 - INFO - __main__ -     epoch = 3.0\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:29 - INFO - __main__ -     eval_accuracy = 0.7892156862745098\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:29 - INFO - __main__ -     eval_combined_score = 0.8183667083854819\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:29 - INFO - __main__ -     eval_f1 = 0.847517730496454\u001b[0m\n",
      "\u001b[34m12/31/2020 08:29:29 - INFO - __main__ -     eval_loss = 0.4569968283176422\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-12-31 08:29:40 Uploading - Uploading generated training model\n",
      "2020-12-31 08:30:16 Completed - Training job completed\n",
      "Training seconds: 357\n",
      "Billable seconds: 357\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}